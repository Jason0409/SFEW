{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-2d44cf2b4e62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 分配 batch data, normalize x when iterate train_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0mflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "datasets = pd.read_csv('SFEW.csv', skiprows=1,\n",
    "                       names=['image', 'label', 'LPQ1', 'LPQ2', 'LPQ3', 'LPQ4', 'LPQ5', 'PHOG1', 'PHOG2', 'PHOG3',\n",
    "                              'PHOG4', 'PHOG5'])\n",
    "# print(datasets.label.value_counts())\n",
    "\n",
    "tr, test = train_test_split(datasets, stratify=datasets.label, test_size=0.2)\n",
    "# print(tr.label.value_counts())\n",
    "# print(test.label.value_counts())\n",
    "tr, val = train_test_split(tr, stratify=tr.label, test_size=0.2)\n",
    "# print(val.label.value_counts())\n",
    "\n",
    "# traning set\n",
    "tr_features_LPQ = [[lp1, lp2, lp3, lp4, lp5, ph1, ph2, ph3, ph4, ph5] for\n",
    "                   lp1, lp2, lp3, lp4, lp5, ph1, ph2, ph3, ph4, ph5 in\n",
    "                   zip(tr.LPQ1, tr.LPQ2, tr.LPQ3, tr.LPQ4, tr.LPQ5, tr.PHOG1, tr.PHOG2, tr.PHOG3, tr.PHOG4, tr.PHOG5)]\n",
    "# tr_features_LPQ = [[lp1,lp2,lp3,lp4,lp5] for lp1,lp2,lp3,lp4,lp5  in zip(tr.LPQ1,tr.LPQ2,tr.LPQ3,tr.LPQ4,tr.LPQ5)]\n",
    "tr_features_PHOG = [[ph1, ph2, ph3, ph4, ph5] for ph1, ph2, ph3, ph4, ph5 in\n",
    "                    zip(tr.PHOG1, tr.PHOG2, tr.PHOG3, tr.PHOG4, tr.PHOG5)]\n",
    "tr_labels = [l - 1 for l in tr.label]\n",
    "# print(tr_labels)\n",
    "# from list to tensor\n",
    "tr_features_LPQ_tensor = torch.FloatTensor(tr_features_LPQ)\n",
    "tr_features_PHOG_tensor = torch.FloatTensor(tr_features_PHOG)\n",
    "tr_labels_tensor = torch.FloatTensor(tr_labels)\n",
    "# train data\n",
    "torch_dataset_train_LPQ = Data.TensorDataset(tr_features_LPQ_tensor, tr_labels_tensor)\n",
    "torch_dataset_train_PHOG = Data.TensorDataset(tr_features_PHOG_tensor, tr_labels_tensor)\n",
    "\n",
    "# validation set\n",
    "val_features_LPQ = [[lp1, lp2, lp3, lp4, lp5, ph1, ph2, ph3, ph4, ph5] for\n",
    "                   lp1, lp2, lp3, lp4, lp5, ph1, ph2, ph3, ph4, ph5 in\n",
    "                   zip(val.LPQ1, val.LPQ2, val.LPQ3, val.LPQ4, val.LPQ5, val.PHOG1, val.PHOG2, val.PHOG3, val.PHOG4, val.PHOG5)]\n",
    "# val_features_LPQ = [[lp1, lp2, lp3, lp4, lp5] for lp1, lp2, lp3, lp4, lp5 in\n",
    "#                     zip(val.LPQ1, val.LPQ2, val.LPQ3, val.LPQ4, val.LPQ5)]\n",
    "val_features_PHOG = [[ph1, ph2, ph3, ph4, ph5] for ph1, ph2, ph3, ph4, ph5 in\n",
    "                     zip(val.PHOG1, val.PHOG2, val.PHOG3, val.PHOG4, val.PHOG5)]\n",
    "val_labels = [l - 1 for l in val.label]\n",
    "# from list to tensor\n",
    "val_features_LPQ_tensor = torch.FloatTensor(val_features_LPQ)\n",
    "val_features_PHOG_tensor = torch.FloatTensor(val_features_PHOG)\n",
    "val_labels_tensor = torch.FloatTensor(val_labels)\n",
    "# validation data\n",
    "torch_dataset_val_LPQ = Data.TensorDataset(val_features_LPQ_tensor, val_labels_tensor)\n",
    "torch_dataset_val_PHOG = Data.TensorDataset(val_features_PHOG_tensor, val_labels_tensor)\n",
    "\n",
    "# test set\n",
    "test_features_LPQ = [[lp1, lp2, lp3, lp4, lp5, ph1, ph2, ph3, ph4, ph5] for\n",
    "                     lp1, lp2, lp3, lp4, lp5, ph1, ph2, ph3, ph4, ph5 in\n",
    "                     zip(test.LPQ1, test.LPQ2, test.LPQ3, test.LPQ4, test.LPQ5, test.PHOG1, test.PHOG2, test.PHOG3,\n",
    "                         test.PHOG4, test.PHOG5)]\n",
    "# test_features_LPQ = [[lp1,lp2,lp3,lp4,lp5] for lp1,lp2,lp3,lp4,lp5 in zip(test.LPQ1,test.LPQ2,test.LPQ3,test.LPQ4,test.LPQ5)]\n",
    "test_features_PHOG = [[ph1, ph2, ph3, ph4, ph5] for ph1, ph2, ph3, ph4, ph5 in\n",
    "                      zip(test.PHOG1, test.PHOG2, test.PHOG3, test.PHOG4, test.PHOG5)]\n",
    "test_labels = [l - 1 for l in test.label]\n",
    "# from list to tensor\n",
    "test_features_LPQ_tensor = torch.FloatTensor(test_features_LPQ)\n",
    "test_features_PHOG_tensor = torch.FloatTensor(test_features_PHOG)\n",
    "test_labels_tensor = torch.FloatTensor(test_labels)\n",
    "# test data\n",
    "torch_dataset_test_LPQ = Data.TensorDataset(test_features_LPQ_tensor, test_labels_tensor)\n",
    "torch_dataset_test_PHOG = Data.TensorDataset(test_features_PHOG_tensor, test_labels_tensor)\n",
    "\n",
    "batch_size = 540\n",
    "num_workers = 0\n",
    "\n",
    "train_loader = Data.DataLoader(torch_dataset_train_LPQ, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "val_loader = Data.DataLoader(torch_dataset_val_LPQ, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "test_loader = Data.DataLoader(torch_dataset_test_LPQ, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "\n",
    "\n",
    "# model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 40)\n",
    "        self.batchnorm1d1 = nn.BatchNorm1d(40)\n",
    "        self.fc2 = nn.Linear(40, 80)\n",
    "        self.batchnorm1d2 = nn.BatchNorm1d(80)\n",
    "        self.fc3 = nn.Linear(80, 160)\n",
    "        self.batchnorm1d3 = nn.BatchNorm1d(160)\n",
    "        self.fc4 = nn.Linear(160, 7)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.sof = nn.Softmax()\n",
    "\n",
    "    #         self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.batchnorm1d1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.batchnorm1d2(x)\n",
    "        x = F.relu(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.batchnorm1d3(x)\n",
    "        x = F.relu(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        x = F.relu(x)\n",
    "        y = F.softmax(x)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "model_conv = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_conv.parameters(), lr=0.0004)\n",
    "# optimizer = optim.SGD(model_conv.parameters(), lr=0.01, momentum=0.5)\n",
    "EPOCH = 20001\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (data, target) in enumerate(train_loader):  # 分配 batch data, normalize x when iterate train_loader\n",
    "        flag = True\n",
    "        for e in data.numpy():\n",
    "            if any(np.isnan(e)):\n",
    "                flag = False\n",
    "                break\n",
    "        if not flag:\n",
    "            continue\n",
    "        model_conv.train()\n",
    "        output = model_conv(data)  # output\n",
    "        loss = criterion(output[1], target.long())  # cross entropy loss\n",
    "        optimizer.zero_grad()  # clear gradients for this training step\n",
    "        loss.backward()  # backpropagation, compute gradients\n",
    "        optimizer.step()  # apply gradients\n",
    "        if epoch % 1000 == 0:\n",
    "            test_output = model_conv(val_features_LPQ_tensor)\n",
    "            # print(test_output)\n",
    "            pred_y = torch.max(test_output[1], 1)[1].data.numpy()\n",
    "            # print(pred_y)\n",
    "            # print(val_labels_tensor.data.numpy().astype(int))\n",
    "            accuracy = float((pred_y == val_labels_tensor.data.numpy()).astype(int).sum()) / float(val_labels_tensor.size(0))\n",
    "            print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.numpy(), '| val accuracy: %.2f' % accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 1, 2, 3, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "list1 = [1,2,3]\n",
    "print(list1*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(40, 40)\n",
    "        self.batchnorm1d1 = nn.BatchNorm1d(40)\n",
    "        self.fc2 = nn.Linear(40,80)\n",
    "        self.batchnorm1d2 = nn.BatchNorm1d(80)\n",
    "        self.fc3 = nn.Linear(80,160)\n",
    "        self.batchnorm1d3 = nn.BatchNorm1d(160)\n",
    "        self.fc4 = nn.Linear(160,320)\n",
    "        self.batchnorm1d4 = nn.BatchNorm1d(320)\n",
    "        self.fc5 = nn.Linear(320,160)\n",
    "        self.batchnorm1d5 = nn.BatchNorm1d(160)\n",
    "        self.fc6 = nn.Linear(160,7)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.sof = nn.Softmax()\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.batchnorm1d1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.batchnorm1d2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.batchnorm1d3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.batchnorm1d4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.batchnorm1d5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc6(x)\n",
    "        y = self.sof(x)\n",
    "        return x,y\n",
    "    \n",
    "loss 1.1654"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
